# ðŸš€ 1. Install dependencies
!pip install -q transformers datasets peft trl accelerate bitsandbytes huggingface_hub

# ðŸš€ 2. Upload your training data (fine_tune.jsonl)
from google.colab import files
uploaded = files.upload()  # Upload your local fine_tune.jsonl here

from huggingface_hub import login

login()

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# âœ… Config
MODEL_NAME = "tiiuae/falcon-rw-1b" #"mistralai/Mistral-7B-Instruct-v0.2"
DATA_PATH = "fine_tune.jsonl"
OUTPUT_DIR = "fine_tuned_model"

# âœ… Load dataset
dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# âœ… Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# âœ… 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

# âœ… Load base model with quantization
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)

# âœ… PEFT config (LoRA)
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)

# âœ… Tokenize data
def tokenize(example):
    prompt = f"### Input:\n{example['input']}\n\n### Output:\n{example['output']}"
    return tokenizer(prompt, truncation=True, padding="max_length", max_length=1024)

tokenized = dataset.map(tokenize)

# âœ… Training arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    report_to="none"
)

# âœ… Train
trainer = SFTTrainer(
    model=model,
    train_dataset=tokenized,
    args=training_args
)

trainer.train()

# âœ… Save model
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("âœ… Fine-tuning complete. Model saved to:", OUTPUT_DIR)

!pip install gradio pdfplumber docx2txt

import gradio as gr
import pdfplumber
import docx2txt
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model + tokenizer
model = AutoModelForCausalLM.from_pretrained("fine_tuned_model")
tokenizer = AutoTokenizer.from_pretrained("fine_tuned_model")

# Helper to extract text
def extract_text(file):
    if file.name.endswith(".pdf"):
        with pdfplumber.open(file.name) as pdf:
            return "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
    elif file.name.endswith(".docx"):
        return docx2txt.process(file.name)
    else:
        return "Unsupported file format. Use PDF or DOCX."

# Generate cover letter
def generate_cover_letter(file, job_desc):
    resume_text = extract_text(file)
    prompt = f"### Input:\nResume:\n{resume_text}\n\nJob Responsibilities:\n{job_desc}\n\n### Output:\n"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split("### Output:")[-1].strip()

# UI
iface = gr.Interface(
    fn=generate_cover_letter,
    inputs=[
        gr.File(label="Upload Resume (PDF or DOCX)"),
        gr.Textbox(label="Paste Job Responsibilities", lines=10),
    ],
    outputs=gr.Textbox(label="Generated Cover Letter", lines=12),
    title="ðŸ“„ Cover Letter Generator (Fine-Tuned)",
    description="Upload your resume and paste job description to generate a custom cover letter using your fine-tuned model."
)

iface.launch()
